{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e75db50",
   "metadata": {},
   "source": [
    "# DRLND - P1 - Navigation : Report\n",
    "In this report, I am going to present about the environment and the algorithms that I have used to solve the navigation problem where the agent is to collect bananas.\n",
    "\n",
    "Environment\n",
    "The project uses an environment built in Unity. The nvironment contain brains which are responsible for deciding the actions of their associated agents. Below are the characteristics of the environment.\n",
    "\n",
    "Unity brain name: BananaBrain\n",
    "Vector Observation space type: continuous\n",
    "Vector Observation space size (per agent): 37\n",
    "Number of stacked Vector Observation: 1\n",
    "Vector Action space type: discrete\n",
    "Vector Action space size (per agent): 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03e67e",
   "metadata": {},
   "source": [
    "## Solution\n",
    "This implementation solved the environment in 223 episodes. The suggested benchmark for completing the exercise was to solve the environment in fewer than 1800 episodes.\n",
    "\n",
    "The key to reducing the number of training episodes was to add a decaying epsilon-greedy policy to the basic DQN solution provided in earlier assignments. This ensured that early episodes focused on exploring the state space while transitioning fairly quickly (within a couple hundred episodes) to a policy based on experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8b3fd0",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Framework\n",
    "The reinforcement learning (RL) framework models the interactions between:\n",
    "\n",
    "an environment that provides rewards based on its state;\n",
    "an agent seeking to maximize those rewards;\n",
    "a policy that the agent uses to select an action in each state.\n",
    "The RL solution generates the policy from an action-value function, also called a Q-function, that estimates the value of each action in a given state. The Q-function is used to estimate the action with the highest reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229728ef",
   "metadata": {},
   "source": [
    "## Learning algorithm\n",
    "The DQN uses two neural networks to estimate the action-value function: a local network representing the current action-value function estimate, and a target network that represents the supervised learning targets. Every n steps, the current reward is added to the target network's predicted value of the next state to generate the targets. The weights of the local network are trained to minimize loss relative to the targets.\n",
    "\n",
    "The algorithm is an online method that updates the q-network incrementally while playing, rather than holding updates until the end of an episode. This implementation uses an epsilon-greedy policy in which the agent chooses a random action with probability ε and the greedy action with probability 1 - ε."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac9271c",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "The solution utilizes the following hyperparameters:\n",
    "\n",
    "* BUFFER_SIZE\t|  100000\t\n",
    "* BATCH_SIZE\t|  64\n",
    "* GAMMA         |  0.99\n",
    "* LR            |\n",
    "* TAU           |\n",
    "* UPDATE_EVERY  \n",
    "* eps\n",
    "* decay\n",
    "* min_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f751bb87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
